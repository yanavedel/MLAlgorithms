# MLAlgorithms
Some prediction, classification and clustering algorithms on Python (without using Scikit-learn)
<ol>
<li><b>Gradient descent</b> - one of the most popular optimization algorithms practically used in some regression and classification procedures. 
References:
<ul>
<li>https://www.coursera.org/learn/machine-learning - Week 1</li>
<li>https://ru.wikipedia.org/wiki/%D0%93%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA</li>
</ul>
</li>
<li>
<b>Linear regression</b> - the simplest prediction model. References:
<ul>
<li>https://www.coursera.org/learn/machine-learning - Week 1</li>
<li>Bishop's Pattern Recognition and Machine Learning - Chapter 1.1</li>
</ul>
</li>
<li>
<b>Maximum likelihood estimation</b> - probabilistic approach for estimation the unknown parameters of the model. References:
<ul>
<li>https://en.wikipedia.org/wiki/Maximum_likelihood_estimation</li>
</ul>
</li>
<li>
<b>Logistic regression</b> - one of the most popular and simplest classification algorithms. References:
<ul>
<li>https://www.coursera.org/learn/machine-learning - Week 3</li>
<li>Bishop's Pattern Recognition and Machine Learning - Chapter 4.3</li>
</ul>
</li>
<li>
<b>Naive Bayes Classifier</b> - the classification algorithm often used in spam detection. It bases on finding bigger posterior probability from Bayes rule. References:
<ul>
<li>Bishop's Pattern Recognition and Machine Learning - Chapter 1.2 (Bayesian probabilities)</li>
<li>https://www.youtube.com/watch?v=qRJ3GKMOFrE - Andrew Ng lectures(Stanford)</li>
</ul>
</li>
<li>
<b>Gaussian discriminant analysis</b> - another classification approach based on posterior probabilities estimation. It uses the assumption of Gaussian distribution. References:
<ul>
<li>https://www.youtube.com/watch?v=qRJ3GKMOFrE - Andrew Ng lectures(Stanford)</li>
<li>Bishop's Pattern Recognition and Machine Learning - Chapter 2.1 (some notes about Gaussian distribution)</li>
</ul>
</li>
<li>
<b>SVM. Kernels</b> - the classification approach providing separating examples of different classes from estimating and optimizing margins between them. References:
<ul>
<li>https://www.coursera.org/learn/machine-learning - Week 7</li>
<li>Support Vector Machines
Halldor Isak Gylfason
Department of EECS
University of California, Berkeley
Berkeley, CA 94720-1770</li>
<li>CS 229, Autumn 2009
The Simplified SMO Algorithm - the algorithm for providing the conditional optimization in SVM</li>
<li>Bishop's Pattern Recognition and Machine Learning - Chapter 6(examples of kernels)</li>
</ul>
</li>
<li>
<b>K-means</b> - one of the most known and simplest clustering algorithms. References:
<ul>
<li>https://en.wikipedia.org/wiki/K-means_clustering</li>
<li>Bishop's Pattern Recognition and Machine Learning - Chapter 9.1</li>
</ul>
</li>
<li>
<b>PCA</b> - the preprocessing procedure for converting a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables. References:
<ul>
<li>https://en.wikipedia.org/wiki/Principal_component_analysis</li>
<li>https://www.coursera.org/learn/machine-learning - Week 8</li>
<li>Bishop's Pattern Recognition and Machine Learning - Chapter 12</li>
</ul>
</li>
<li>
<b>ICA</b> - the algorithm for decomposition a multivariate signal into independent non-Gaussian signals. References:
<ul>
<li>https://en.wikipedia.org/wiki/Independent_component_analysis</li>
<li>https://en.wikipedia.org/wiki/FastICA</li>
<li>Independent Component Analysis􀀀 A Tutorial
Aapo Hyv􀀀rinen and Erkki Oja
Helsinki University of Technology
Laboratory of Computer and Information Science
PO Box  FIN Espoo Finland</li>
</ul>
</li>
</ol>
